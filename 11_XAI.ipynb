{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luferIPCA/MIA-MLA-24-25/blob/main/11_XAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sFdu_LGitF5"
      },
      "outputs": [],
      "source": [
        "#!Begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1kf9qddTM4D"
      },
      "source": [
        "# Masters' in Applied Artificial Intelligence\n",
        "## Machine Learning Algorithms Course\n",
        "\n",
        "Notebooks for the MLA course\n",
        "\n",
        "by [*lufer*](mailto:lufer@ipca.pt)\n",
        "\n",
        "(ver 2.0)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yc6mD0jVeWN"
      },
      "source": [
        "# ML Modelling - Part IV - Explainable Artificial Models\n",
        "\n",
        "**Contents**:\n",
        "\n",
        "1. **White-Box versus Black-Box Models**\n",
        "2. **Case Study**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook promotes Explainable Artiticial Intelligence!"
      ],
      "metadata": {
        "id": "iAEg0vfdoiFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explainable Artificial Intelligent models\n",
        "\n",
        "\"...help extract insight and clarity regarding how these algorithms are performing and why one prediction is made over another...\"\n",
        "\n",
        "\n",
        "* White Box Models\n",
        "\n",
        "  - possible to explain\n",
        "\n",
        "* Black Box models\n",
        "  - hard to explain\n",
        "  - ex: Deep Learning models"
      ],
      "metadata": {
        "id": "VliLb9O97d2d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP-NymupVL02"
      },
      "source": [
        "# Environment preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3Rm857IVoPe"
      },
      "source": [
        "**Install necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install libraries for trainning for Explainable AI\n",
        "!pip install lime\n",
        "!pip install interpret\n",
        "#!pip install eli5\n",
        "!pip install shap"
      ],
      "metadata": {
        "id": "6Ey1bZFNFtdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install necessary libraries**"
      ],
      "metadata": {
        "id": "D2Y1PxvbJx-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing Libraries\n",
        "\n",
        "#general\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#for AI model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import IPython  #visualiztion\n",
        "\n",
        "#for XAI\n",
        "\n",
        "#eli5 - models Debugging - require a minor sklearn release\n",
        "#import eli5\n",
        "#from eli5 import show_prediction\n",
        "\n",
        "#shap - SHapley Additive exPlanations (SHAP)\n",
        "import shap\n",
        "\n",
        "#lime - Local Interpretable Model-Agnostic Explanations\n",
        "import lime.lime_tabular\n",
        "\n",
        "#interpret\n",
        "from interpret import set_visualize_provider\n",
        "from interpret.provider import InlineProvider\n",
        "from interpret.glassbox import ExplainableBoostingClassifier\n",
        "from interpret import show"
      ],
      "metadata": {
        "id": "gZ5wYzX3Hj3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "print(f\"Last updated: {datetime.datetime.now()}\")"
      ],
      "metadata": {
        "id": "wuXym8xInfIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDLxcgMwJEYA"
      },
      "source": [
        "**Mounting Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxFY0ypTJJK9"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# it will ask for your google drive credentiaals\n",
        "drive.mount('/content/gDrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data"
      ],
      "metadata": {
        "id": "LqOOL0uDgaqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filePath=\"/content/gDrive/MyDrive/Colab Notebooks/MIA - ML - 2024-2025/Datasets/\"\n",
        "df = pd.read_csv(filePath+\"Credit.csv\")"
      ],
      "metadata": {
        "id": "Vi0dyX6wkM0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check dataset size\n",
        "df.shape"
      ],
      "metadata": {
        "id": "vXNCk55Gk5Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "8YDBNx2TZmMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Features and Target"
      ],
      "metadata": {
        "id": "U5oj6G_zZ7MO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = df.drop(columns=[\"class\"])\n",
        "target = df[\"class\"]\n",
        "# Reassign feature names explicitly (optional, but ensures consistency)\n",
        "#features = pd.DataFrame(features, columns=features.columns)\n",
        "\n",
        "#or\n",
        "#features = df.iloc[:, :-1].values  #except last column\n",
        "#target = df.iloc[:,-1].values      #last column\n",
        "#NOTE: in this case, features and target come as a NumPy arrays"
      ],
      "metadata": {
        "id": "ErpH69KsvZ4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check features\n",
        "features.dtypes"
      ],
      "metadata": {
        "id": "Nk36cXDCZ5Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "id": "PtrDzVwDW8K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.shape[1]"
      ],
      "metadata": {
        "id": "XZ0kwCA8Xc81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert all Categorical values in Numerical"
      ],
      "metadata": {
        "id": "98KaV6V-XUaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert clategorial features in numerical\n",
        "labelencoder = LabelEncoder()\n",
        "features = features.apply(lambda col: labelencoder.fit_transform(col) if col.dtype == 'object' else col)\n",
        "features"
      ],
      "metadata": {
        "id": "a53M_iVAozmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert clategorial target in numerical\n",
        "#Use a different LabelEncoder for the target\n",
        "target_encoder = LabelEncoder()\n",
        "#target=target_encoder.fit_transform(target)\n",
        "target = pd.Series(target_encoder.fit_transform(target), name=\"class\")  # Restore column name"
      ],
      "metadata": {
        "id": "1drVzug4v9yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "id": "_YOlO3YeaUyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split Train+Test: 70+30\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(features, target, test_size=0.3)"
      ],
      "metadata": {
        "id": "qJHpxeT1wPjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain"
      ],
      "metadata": {
        "id": "fYAY9sXz1APz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the model\n",
        "#Xtrain = pd.DataFrame(Xtrain, columns=feature_names)  # Use actual feature names\n",
        "m = RandomForestClassifier(n_estimators=1000)\n",
        "m.fit(Xtrain, ytrain)"
      ],
      "metadata": {
        "id": "e2tWxVDhwEw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring XAI Tools"
      ],
      "metadata": {
        "id": "Mo2b2PYSfcBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lime\n",
        "\n",
        "LIME (Local Interpretable Model-agnostic Explanations)\n",
        "\n",
        "Works mainly for local instances!"
      ],
      "metadata": {
        "id": "4B0dAawFxxv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtest[0:1]"
      ],
      "metadata": {
        "id": "zJLX4y0pfrQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ytest[:1]"
      ],
      "metadata": {
        "id": "2ABl2OL2fvlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check all column names\n",
        "list(df)"
      ],
      "metadata": {
        "id": "D9fHeIEgy2ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check train columns names\n",
        "Xtrain.columns\n",
        "#same as\n",
        "#list(df)[0:20]"
      ],
      "metadata": {
        "id": "_j7OjIBBrB24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lime require Numpy arrays\n",
        "#Ensure Xtrain is converted to NumPy array\n",
        "Xtrain_np = Xtrain.values  # Convert DataFrame to NumPy array\n",
        "\n",
        "expl = lime.lime_tabular.LimeTabularExplainer(Xtrain_np, feature_names=list(Xtrain.columns),class_names=\"class\")\n",
        "#prever = lambda x: m.predict_proba(x).astype(float)  #\"x\" loose column names\n",
        "prever = lambda x: m.predict_proba(pd.DataFrame(x, columns=Xtrain.columns)).astype(float)\n",
        "\n",
        "#Explain the first instance - remember that lime is local!\n",
        "#Ensure the Xtest instance is converted to NumPy array\n",
        "Xtest_np = Xtest[0:1].values\n",
        "exp = expl.explain_instance(Xtest_np[0], prever, num_features=5)\n",
        "exp.show_in_notebook(show_all=True)\n"
      ],
      "metadata": {
        "id": "syqaW3ATxz0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dalex\n",
        "\n",
        "\n",
        "**Dalex: moDel Agnostic Language for Exploration and eXplanation**\n",
        "\n",
        "- Helps to analyze and interpret black-box models by providing tools to understand how features influence predictions.\n",
        "- Is model agnóstic too\n",
        "- Supports Global and Local explanation\n",
        "\n",
        "See [Dalex Explanatory Model Analysis](https://medium.com/@ModelOriented/dalex-v-1-0-and-the-explanatory-model-analysis-419585a4ba91)"
      ],
      "metadata": {
        "id": "UCnXTZo9kxrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**The explanaion analysis can me made on Xtrain or Xtest:**\n",
        "\n",
        "**Analysing Xtest**\n",
        "\n",
        "*exp = dx.Explainer(model, Xtest, ytest.astype(float))*\n",
        "\n",
        "- Better for evaluation: How the model behaves on unseen data.\n",
        "- Checks for overfitting: Explaining test set predictions, shows if the model generalizes well.\n",
        "- Common practice: To avoid biased conclusions.\n",
        "\n",
        "\n",
        "**Analysing Xtrain**\n",
        "\n",
        "*exp = dx.Explainer(model, Xtrain, ytrain.astype(float))*\n",
        "\n",
        "- Useful for debugging model behavior: How the model learned patterns from training data.\n",
        "- For feature engineering analysis: Ccan reveal model reliance to new features\n",
        "- Might overfit explanations: explanations may reflect patterns the model memorized rather than general trends.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3ILeuLUdI2qP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dalex"
      ],
      "metadata": {
        "id": "WnUBtf8ug-S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the dalex explainer\n",
        "import dalex as dx\n",
        "\n",
        "explainer = dx.Explainer(m, Xtrain, ytrain)"
      ],
      "metadata": {
        "id": "94tHmNQShFgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting with the model\n",
        "# Predict on the test data\n",
        "predictions = explainer.predict(Xtest)\n",
        "\n",
        "# check predictions\n",
        "print(predictions[:10])  # Show first 10 predictions"
      ],
      "metadata": {
        "id": "rzjmniSgAULe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Global Explanation\n",
        "\n",
        "*exp.model_parts()*"
      ],
      "metadata": {
        "id": "8ymb21Qyh2cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "# Compute feature importance (global explanation)\n",
        "global_explanation = explainer.model_parts()\n",
        "\n",
        "# Plot the global explanation\n",
        "global_explanation.plot()"
      ],
      "metadata": {
        "id": "cRfDObWEhm5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "\n",
        "To avoid the warning \"RandomForestClassifier was fitted without feature names\"\n",
        "prepare the model differently:\n",
        "\n",
        "```\n",
        "# import pandas as pd\n",
        "features:names=Xtrain.columns\n",
        "Xtrain = pd.DataFrame(Xtrain, columns=feature_names)  # Use actual feature names\n",
        "m.fit(Xtrain, ytrain)\n",
        "```"
      ],
      "metadata": {
        "id": "l1YZoru2kTsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instances Explanation\n",
        "\n",
        "*exp.predict_parts()*"
      ],
      "metadata": {
        "id": "bkLvIhAphzzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select an instance to explain (e.g., the first row in Xtrain)\n",
        "instance = Xtrain.iloc[0]\n",
        "#or int the Ttest part\n",
        "#instance = Xtest.iloc[0]\n",
        "\n",
        "# Compute local explanation using Shapley values\n",
        "localExplanation = explainer.predict_parts(instance, type='shap')\n",
        "\n",
        "#try with different explanation types:\n",
        "#type='break_down_interactions'\n",
        "#type='break_down'\n",
        "\n",
        "# Plot the local explanation\n",
        "localExplanation.plot()\n",
        "#try\n",
        "#localExplanation.result"
      ],
      "metadata": {
        "id": "LBeBdtymhymU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict_profile()\n",
        "#Analyze how predictions change when varying a single feature while keeping others constant\n",
        "\n",
        "#explainer = dx.Explainer(m, Xtrain, ytrain)\n",
        "# ✅ Fix: Convert target variable to float\n",
        "explainer = dx.Explainer(m, Xtest, ytest.astype(float))  # <-- Fix here\n",
        "\n",
        "# Generate a profile for \"credit_history\"\n",
        "profile = explainer.predict_profile(Xtest, variables=\"credit_history\")\n",
        "\n",
        "# Plot the profile\n",
        "profile.plot()"
      ],
      "metadata": {
        "id": "5awOmxrYHMDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "More:\n",
        "\n",
        "*exp.predict_diagnostic()*\n",
        "\n",
        "Checking model stability; Understanding prediction distributions: Identifying potential biases\n",
        "\n",
        "*exp1.predict_diagnostic(exp2*)\n",
        "\n",
        "Compare diagnostics between two models\n",
        "\n"
      ],
      "metadata": {
        "id": "BOwEC9OqGBbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shap\n",
        "\n",
        "SHAP (SHapley Additive exPlanations)\n",
        "\n",
        "- Supports Global and Local explanation\n",
        "\n",
        "\n",
        "See [How to interpret and explain your machine learning models using SHAP values](https://m.mage.ai/how-to-interpret-and-explain-your-machine-learning-models-using-shap-values-471c2635b78e)"
      ],
      "metadata": {
        "id": "vyLYglf42Vnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain.head()"
      ],
      "metadata": {
        "id": "SvOgfzOwQa0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain.columns"
      ],
      "metadata": {
        "id": "OGlZqkK5QoKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the explainer\n",
        "\n",
        "Xtrain2 = Xtrain.astype(float)  #preserv original Xtrain, converting all values to float. Shpa requires it!\n",
        "explainer = shap.Explainer(m, Xtrain2)\n",
        "\n",
        "shap_values = explainer.shap_values(Xtest)    #2d array\n"
      ],
      "metadata": {
        "id": "Czh7Iq3e2YOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shap_values\n"
      ],
      "metadata": {
        "id": "8g1wKdIDRMGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Explanations for each class\n",
        "#column names\n",
        "columns=Xtrain.columns\n",
        "shap.summary_plot(shap_values, Xtest, feature_names=columns, plot_type='bar')\n",
        "#shap.summary_plot(shap_values, Xtest)\n",
        "\n",
        "shap.force_plot(explainer.expected_value[1], shap_values[1])  #explain class \"1\"\n",
        "\n",
        "shap.initjs()\n",
        "#check for class \"0\"\n"
      ],
      "metadata": {
        "id": "6QwYlzGKQTVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "\n",
        "- The char shows the most relevant features\n",
        "- In this chart, red color means higher value of a feature. Blue means lower value of a feature.\n",
        "- We can get the general sense of features’ directionality impact based on the distribution of the red and blue dots!"
      ],
      "metadata": {
        "id": "oH0Ebw1YTdCT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpret"
      ],
      "metadata": {
        "id": "5oJckNsj5QHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_visualize_provider(InlineProvider())\n",
        "ebm = ExplainableBoostingClassifier(feature_names=Xtrain.columns)\n",
        "ebm.fit(Xtrain, ytrain)\n",
        "globalExplanation = ebm.explain_global()\n",
        "show(globalExplanation)"
      ],
      "metadata": {
        "id": "ylM0PAtN5S8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References"
      ],
      "metadata": {
        "id": "kK3-qlb-E3ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  [Explainable AI - Understanding and Trusting Machine Learning Models](https://www.datacamp.com/tutorial/explainable-ai-understanding-and-trusting-machine-learning-models)  \n",
        "*  [Why is explainability important?](https://xai-tutorials.readthedocs.io/en/latest/_xai/importance.html)\n",
        "*  [SHapley Additive exPlanations (SHAP)](https://xai-tutorials.readthedocs.io/en/latest/_model_agnostic_xai/shap.html)\n",
        "*  [Local Interpretable Model-Agnostic Explanations (LIME)](https://xai-tutorials.readthedocs.io/en/latest/_model_agnostic_xai/lime.html)\n",
        "*  [Eli5 (Explain it like I am 5) Model Explainability in Python](https://medium.com/chat-gpt-now-writes-all-my-articles/eli5-explain-it-like-i-am-5-model-explainability-in-python-d4922f021037)\n",
        "*  [ELI5’s documentation!](https://eli5.readthedocs.io/en/latest/overview.html)\n",
        "*  [Techniques for Interpreting and Explaining ML Models](https://www.markovml.com/blog/model-interpreting)\n",
        "\n",
        "Applied\n",
        "\n",
        "*  [Explanatory Model Analysis (Section 3.2.2)](https://ema.drwhy.ai/)"
      ],
      "metadata": {
        "id": "VC5zdp4pFG_i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i6nDKkItGJ2m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}